\section{Probabilistic Analysis of Core Sketch Error}
Let us introduce the orthonormal matrix $\mathbf{Q}_n^\bot$ whose range is complementary to that of $\mathbf{Q}_n$ for each n: $\mathbf{Q}_n^\bot (\mathbf{Q}_n^\bot)^\top = \mathbf{I} - \mathbf{Q}_n\mathbf{Q}_n^\top$. Next, we denote 
\begin{equation}
\begin{aligned}
\mathbf{\Phi}_n^Q = \mathbf{\Phi}_n \mathbf{Q}_n  ~~~~\mathbf{\Phi}_n^{Q^\bot} = \mathbf{\Phi}_n \mathbf{Q}_n^\bot.  
\end{aligned}
\end{equation}
Apparently, conditional on $\mathbf{Q}_n, \mathbf{Q}_n^\bot$, $\mathbf{\Phi}_n^Q, \mathbf{\Phi}_n^{Q^\bot}$ are independent with each other. 


\subsection{Decomposition of Core Sketch Error}
This session, we show a decomposition formula for core sketch error.  
\begin{lem}
\label{lem:core_error_decomposition}
Assume $\mathbf{\Phi}_n$ has full column rank (this holds with probability 1 in fact). Then
\begin{equation}
\mathscr{W} - \mathscr{X}\times_1 \mathbf{Q}_1^\top \dots \times_N \mathbf{Q}_N^\top = 
\sum_{(i_1,\dots, i_N) \in \{0,1\}^N, \prod_{j=1}^N i_j \neq 0} \mathscr{Y}_{i1\dots i_N}, 
\end{equation}
where 
\begin{equation}
\label{eq:def_each_part}
\begin{aligned}
\mathscr{Y}_{i1\dots i_N} &= \mathscr{X}\times_1 \left(\mathbbm{1}_{i_1=0}\mathbf{Q}_1^\top + \mathbbm{1}_{i_1=0}+\mathbbm{1}_{i_1=1}(\mathbf{\Phi}_1^Q)^\dag  \mathbf{\Phi}_1^{Q^\bot}(\mathbf{Q}_1^\bot)^\top) \right)\\
&\times \dots \times_N \left(\mathbbm{1}_{i_N=0}\mathbf{Q}_1^\top + \mathbbm{1}_{i_N=0}+\mathbbm{1}_{i_1=1}(\mathbf{\Phi}_N^Q)^\dag  \mathbf{\Phi}_N^{Q^\bot}(\mathbf{Q}_1^\bot)^\top) \right).
\end{aligned}
\end{equation}
\end{lem}


\begin{proof}
We could write $\mathscr{W}$ as 
\begin{equation}
\begin{aligned}
&\mathscr{W} = \mathscr{Z}\times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag \\
& = (\mathscr{X} -  \tilde{\mathscr{X}})\times_1 \mathbf{\Phi}_1 \times \cdots \times_N \mathbf{\Phi}_N  \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag 
\\
&+ \tilde{\mathscr{X}}\times_1 \mathbf{\Phi}_1 \times \cdots \times_N \mathbf{\Phi}_N \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag.
\end{aligned}
\end{equation}
Then, we could simplify the second term as 
\begin{equation}
\begin{aligned}
&\tilde{\mathscr{X}}\times_1 \mathbf{\Phi}_1 \times \cdots \times_N \mathbf{\Phi}_N \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag   \\
& = \mathscr{X}\times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \mathbf{\Phi}_1\mathbf{Q}_1\mathbf{Q}_1^\top \times \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag \mathbf{\Phi}_N\mathbf{Q}_N\mathbf{Q}_N^\top\\
& = \mathscr{X}\times_1 \mathbf{Q}_1^\top \times \cdots \times_N \mathbf{Q}_N^\top.
\end{aligned}
\end{equation}
The last equation comes from the fact that for a matrix $\mathbf{A}$ with shape $s\times k$, if $s>k$, $A^\dag A = I_k$. Therefore
\begin{equation}
\begin{aligned}
&\mathscr{W} = (\mathscr{X} -  \tilde{\mathscr{X}})\times_1 \mathbf{\Phi}_1 \times \cdots \times_N \mathbf{\Phi}_N  \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag \\
&+ \mathscr{X}\times_1 \mathbf{Q}_1^\top \times \cdots \times_N \mathbf{Q}_N^\top.
\end{aligned}
\end{equation}
Then the error could be decomposed as 
\begin{equation}
\label{eq:core_err_decom}
\begin{aligned}
&(\mathscr{X} -  \tilde{\mathscr{X}})\times_1 \mathbf{\Phi}_1 \times \cdots \times_N \mathbf{\Phi}_N  \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag \\
& =(\mathscr{X} -  \tilde{\mathscr{X}})\times_1 (\mathbf{\Phi}_1\mathbf{Q}_1)^\dag \mathbf{\Phi}_1 \times \cdots \times_N (\mathbf{\Phi}_N\mathbf{Q}_N)^\dag \mathbf{\Phi}_N \\
& =  (\mathscr{X} -  \tilde{\mathscr{X}}) \times_1 (\mathbf{\Phi}_1\mathbf{Q}_1)^\dag \mathbf{\Phi}_1(\mathbf{Q}_1\mathbf{Q}_1^\top + \mathbf{Q}_1^\bot (\mathbf{Q}_1^\bot)^\top)\dots \times_N (\mathbf{\Phi}_N\mathbf{Q}_N)^\dag \mathbf{\Phi}_N(\mathbf{Q}_N\mathbf{Q}_N^\top + \mathbf{Q}_N^\bot (\mathbf{Q}_N^\bot)^\top)\\
& = (\mathscr{X} -  \tilde{\mathscr{X}}) \times_1 (\mathbf{Q}_1^\top + (\mathbf{\Phi}_1^Q)^\dag  \mathbf{\Phi}_1^{Q^\bot}(\mathbf{Q}_1^\bot)^\top)\dots \times_N (\mathbf{Q}_N^\top + (\mathbf{\Phi}_N^Q)^\dag  \mathbf{\Phi}_N^{Q^\bot}(\mathbf{Q}_N^\bot)^\top).
\end{aligned}
\end{equation}
Then we need to sum all $2^N$ terms. To simply the summation, we claim following
\begin{enumerate}
\item $(\mathscr{X} - \tilde{\mathscr{X}})\times_1 \mathbf{Q}_1^\top\dots \times_N \mathbf{Q}_N^\top = 0$,
\item for any $1\le n\le N$, $\tilde{\mathscr{X}}\times_n (\mathbf{\Phi}_n^Q)^\dag  \mathbf{\Phi}_n^{Q^\bot}(\mathbf{Q}_n^\bot)^\top =  0$.
\end{enumerate}
Here $0$ means a tensor with all zero elements. There two claims could be got straightly from exchange rule of mode product given by \ref{eq: tensor_product_mul_exchangable}. Then Follow the definition of $\mathscr{Y}_{i_1\dots i_N}$ in \eqref{eq:def_each_part},
we could write \eqref{eq:core_err_decom} as 
\begin{equation}
\sum_{(i_1,\dots, i_N) \in \{0,1\}^N, \prod_{j=1}^N i_j \neq 0} \mathscr{Y}_{i1\dots i_N},
\end{equation}
which completes the proof.
\end{proof}
\subsection{Probabilistic Core Error Bound}
In this section, we derive the probabilistic error bound based on deterministic error shown in lemma \ref{lem:core_error_decomposition}. 
\begin{lem}
\label{lemma:err_core_sketch}
If $s>2k$, each element of $\Phi_n, \Omega_n$ is from standard Gaussian distribution for all $n$, then for any natural number $1\le \rho<k$, 
\begin{equation}
\mathbb{E} \|\mathscr{W} - \mathscr{X}\times_1 \mathbf{Q}_1^\top \dots \times_N \mathbf{Q}_N^\top\|_F^2 \le \frac{k}{s-k-1} \left[ \sum_{n=1}^N \left(1+\frac{k}{k-\rho-1}\right)(\tau^{(n)}_\rho)^2\right]. 
\end{equation}
\end{lem}
\begin{proof}
It suffices to show that 
\begin{equation}
\mathbb{E}\left[ \|\mathscr{W} - \mathscr{X}\times_1 \mathbf{Q}_1^\top \dots \times_N \mathbf{Q}_N^\top\|_F^2 \mid \mathbf{\Omega}_1, \cdots, \mathbf{\Omega}_N \right] \le \frac{k}{s-k-1}  \|\mathscr{X} - \tilde{\mathscr{X}}\|_F^2, 
\end{equation}
then we could take expectation respect to $\mathbf{\Omega}_1, \cdots,  \mathbf{\Omega}_N$ and apply lemma \ref{lemma: compression_error} to finish the proof. Here we use the fact that $\{\mathbf{\Omega_n}, 1\le n\le N\}$ are independent with $\{\mathbf{\Phi_n}, 1\le n\le N\}$ and randomness of $\mathbf{Q}_n$ solely comes from $\mathbf{\Omega}_n$. \par 
Consider the conditional expectation for each $\mathscr{Y}_n$. Given $\mathbf{\Omega}_1,\dots, \mathbf{\Omega}_N$, 
$\mathbf{\Phi}_n^Q$ is independent with $\mathbf{\Phi}_n^{Q^\bot}$, thus applying lemma \ref{lemma:expectation_inverse_gaussian} we could get 
\begin{equation}
\mathbb{E} \left[ \|\mathscr{Y}_{i_1\dots i_N}\|_F^2 \mid \mathbf{\Omega}_1 \cdots \mathbf{\Omega}_N \right] = \left(\frac{k}{s-k-1}\right)^{\sum_{j=1}^N i_j} \|\mathscr{B}_{i_1\dots i_N}\|_F^2 \le \left(\frac{k}{s-k-1}\right) \|\mathscr{B}_{i_1\dots i_N}\|_F^2.
\end{equation}
Here we use the fact that we assume $s>2k$ and define $\mathscr{B}_{i_1\dots i_N}$ as
\begin{equation}
\mathscr{B}_{i_1\dots i_N} = \mathscr{X}\times_1 (\mathbbm{1}_{i_1=0}\mathbf{Q}_1 + \mathbbm{1}_{i_1=1}\mathbf{Q}_1^\bot)\cdots\times_N(\mathbbm{1}_{i_N=0}\mathbf{Q}_1 + \mathbbm{1}_{i_N=1}\mathbf{Q}_N^\bot).
\end{equation}
It is not hard to see that
\begin{equation}
\begin{aligned}
&\sum_{(i_1, \dots, i_N) \in \{0,1\}^N} \mathscr{B}_{i_1\dots i_N} = \mathscr{X}\times_1(\mathbf{Q}_1^\top + (\mathbf{Q}_1^\bot)^\top)\cdots\times_N (\mathbf{Q}_N^\top+(\mathbf{Q}_N^\bot)^\top)\\
& = \mathscr{X}\times_1(\mathbf{Q}_1\mathbf{Q}_1^\top + \mathbf{Q}_1^\bot(\mathbf{Q}_1^\bot)^\top)\cdots\times_N (\mathbf{Q}_N \mathbf{Q}_N^\top+\mathbf{Q}_N^\bot(\mathbf{Q}_N^\bot)^\top) = \mathscr{X}.
\end{aligned}
\end{equation}
Let $q_1, q_2 \in \{0,1\}^N$ be the index(binary) vector of length $N$. For different index $q_1, q_2$, at least there exists $1\le r\le N$, their r-th element is different. Without losing generality, $q_1(r) = 0$ and  $q_2(r)=1$, 
\begin{equation}
\langle \mathscr{B}_{q_1}, \mathscr{B}_{q_2}\rangle = \langle  \mathbf{B}^{(r)}_{q_1}, \mathbf{B}^{(r)}_{q_2} \rangle = \langle \dots \mathbf{Q}_r^\top \mathbf{Q}_r^\bot \dots\rangle  = 0. 
\end{equation}
Then by Pythagorean theorem, 
\begin{equation}
\sum_{(i_1,\dots, i_N) \in \{0,1\}^N} \|\mathscr{B}_{i_1\dots i_N}\|^2 = \|\mathscr{X} \|_F^2, 
\end{equation}
which indicates that 
\begin{equation}
\sum_{(i_1,\dots, i_N) \in \{0,1\}^N, \prod_{j=1}^N i_j \neq 0} \|\mathscr{B}_{i_1\dots i_N}\|_F^2 = \|\mathscr{X} - \tilde{\mathscr{X}}\|_F^2. 
\end{equation}
Putting all these together, 
\begin{equation}
\begin{aligned}
&\mathbb{E}\left[ \|\mathscr{W} - \mathscr{X}\times_1 \mathbf{Q}_1^\top \dots \times_N \mathbf{Q}_N^\top\|_F^2 \mid \mathbf{\Omega}_1, \cdots, \mathbf{\Omega}_N \right] \\
& = \sum_{(i_1,\dots, i_N) \in \{0,1\}^N, \prod_{j=1}^N i_j \neq 0} \mathbb{E} \left[\|\mathscr{Y}_{i_1\dots i_N}\|_F^2 \mid \mathbf{\Omega_1}, \dots, \mathbf{\Omega}_N\right]\\
&\le \frac{k}{s-k-1} \left(\sum_{(i_1,\dots, i_N) \in \{0,1\}^N, \prod_{j=1}^N i_j \neq 0} \|\mathscr{B}_{i_1\dots i_N}\|_F^2 \right)\\
&= \frac{k}{s-k-1} \|\mathscr{X} - \tilde{\mathscr{X}}\|_F^2. 
\end{aligned}
\end{equation}
This completes the proof. 
\end{proof}


\iffalse
\section{Probabilistic Analysis of Core Tensor}
We first introduce a lemma which may assist us to apply lemma \ref{lemma:expectation_inverse_gaussian} in lemma \ref{lemma:probabilistic_err_linkage_tensor}. 
\begin{lem}
\label{lemma:indepence_for_pseduo_inverse}
Follow the definition of $\mathbf{\Omega}_n$ and $\mathbf{Q}_n$, 
for any $1\le n\le N$, $\mathbf{\Phi}_n \mathbf{Q}_n$ and 
$\mathbf{\Phi}_n (\mathbf{I} - \mathbf{Q}_n\mathbf{Q}_n^\top)$ are conditionally independent given $\mathbf{Q}_n$. 
\begin{proof}
Since both $(\mathbf{\Phi}_n \mathbf{Q}_n)$ and $(\mathbf{\Phi}_n (\mathbf{I} -\mathbf{Q}_n\mathbf{Q}_n^\top))$ are both Gaussian random matrices, it suffices to show that the covariance matrix between $\mbox{vec}(\mathbf{\Phi}_n \mathbf{Q}_n)$ and $\mbox{vec}(\mathbf{\Phi}_n (\mathbf{I} -\mathbf{Q}_n\mathbf{Q}_n^\top))$ given $\mathbf{Q}_n$ , is the identity. Note: $\mbox{vec}(AB) = (B^\top \otimes I)\mbox{vec}(A)$. 
\begin{equation}
\begin{aligned}
&\cov(\mbox{vec}(\mathbf{\Phi}_n \mathbf{Q}_n), \mbox{vec}(\mathbf{\Phi}_n (\mathbf{I} -\mathbf{Q}_n\mathbf{Q}_n^\top)))  = \left(\mathbf{Q}_n^\top \otimes \mathbf{I}_k\right)\left((\mathbf{I} - \mathbf{Q}\mathbf{Q}_n^\top) \otimes \mathbf{I}_k\right)\\
&=\left(\mathbf{Q}_n^\top(\mathbf{I} - \mathbf{Q}_n\mathbf{Q}_n^\top)  \right)\otimes \left(\mathbf{I}_k\right) = \mathbf{I}_{k^2}. 
\end{aligned}
\end{equation}
completes the proof. 
\end{proof}
\end{lem}

\begin{lem}
\label{lemma:probabilistic_err_linkage_tensor}
For any natural number $\rho\le k-1$,
\begin{equation}
\mathbb{E} \|\mathscr{W} - \mathscr{X}\times_1 \mathbf{Q}_1^\top \times \cdots \times_N \mathbf{Q}_N^\top\|_F^2\le 
\left(1+\frac{k}{s-k}\right)^N  \sum_{n=1}^N \left(1+\frac{k}{k-\rho-1}\right)(\tau^{(n)}_\rho)^2. 
\end{equation}
\begin{proof}
To make notation succinct, we borrow the concept and notation of filtration in probability:
\begin{equation}
\mathcal{F}_n = \sigma(\mathbf{\Omega}_1, \mathbf{\Phi}_1, \dots, \mathbf{\Omega}_n, \mathbf{\Phi}_n), 
\end{equation}
where $\sigma(\mathbf{\Omega}_1, \mathbf{\Phi}_1, \dots, \mathbf{\Omega}_n, \mathbf{\Phi}_n)$ is the sigma algebra generated by $\{\mathbf{\Omega}_i, \mathbf{\Phi}_i, 1\le i\le n\}$. Let $\mathcal{F}_0 = \{\Omega, \emptyset\}$,  the trivial sigma algebra. Here $\Omega$ is the whole space. In usage, it just simplifies the writing of the conditional probability:
\begin{equation}
\mathbb{E} (X \mid \mathcal{F}_n) = \mathbb{E} (X\mid \mathbf{Q}_1,\mathbf{\Omega}_1, \dots, \mathbf{Q}_n,\mathbf{\Omega}_n). 
\end{equation}
We could write $\mathscr{W}$ as 
\begin{equation}
\begin{aligned}
&\mathscr{W} = \mathscr{Z}\times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag \\
& = (\mathscr{X} -  \tilde{\mathscr{X}})\times_1 \mathbf{\Phi}_1 \times \cdots \times_N \mathbf{\Phi}_N  \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag 
\\
&+ \tilde{\mathscr{X}}\times_1 \mathbf{\Phi}_1 \times \cdots \times_N \mathbf{\Phi}_N \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag.
\end{aligned}
\end{equation}
Then, we could simplify the second term as 
\begin{equation}
\begin{aligned}
&\tilde{\mathscr{X}}\times_1 \mathbf{\Phi}_1 \times \cdots \times_N \mathbf{\Phi}_N \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag   \\
& = \mathscr{X}\times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \mathbf{\Phi}_1\mathbf{Q}_1\mathbf{Q}_1^\top \times \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag \mathbf{\Phi}_N\mathbf{Q}_N\mathbf{Q}_N^\top\\
& = \mathscr{X}\times_1 \mathbf{Q}_1^\top \times \cdots \times_N \mathbf{Q}_N^\top.
\end{aligned}
\end{equation}
The last equation comes from the fact that for a matrix $\mathbf{A}$ with shape $s\times k$, if $s>k$, $A^\dag A = I_k$. Therefore
\begin{equation}
\begin{aligned}
&\mathscr{W} = (\mathscr{X} -  \tilde{\mathscr{X}})\times_1 \mathbf{\Phi}_1 \times \cdots \times_N \mathbf{\Phi}_N  \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag \\
&+ \mathscr{X}\times_1 \mathbf{Q}_1^\top \times \cdots \times_N \mathbf{Q}_N^\top.
\end{aligned}
\end{equation}
Now, it suffices to bound the first term in the above equation. Applying \eqref{eq: tensor_product_mul_exchangable} and \eqref{eq: tensor_product_association} to the first part, then we could rearrange it as
\begin{equation}
\begin{aligned}
&(\mathscr{X} -  \tilde{\mathscr{X}})\times_1 \mathbf{\Phi}_1 \times \cdots \times_N \mathbf{\Phi}_N  \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag \\
& =(\mathscr{X} -  \tilde{\mathscr{X}})\times_1 (\mathbf{\Phi}_1\mathbf{Q}_1)^\dag \mathbf{\Phi}_1 \times \cdots \times_N (\mathbf{\Phi}_N\mathbf{Q}_N)^\dag \mathbf{\Phi}_N.
\end{aligned}
\end{equation}
Now we use a recursive argument to bound the above equation. Let 
$\mathscr{Y}_0 = (\mathscr{X} -  \tilde{\mathscr{X}})$ and for $1\le n \le N$, 
\begin{equation}
\mathscr{Y}_n =  (\mathscr{X} -  \tilde{\mathscr{X}})\times_1 (\mathbf{\Phi}_1\mathbf{Q}_1)^\dag \mathbf{\Phi}_1 \times \cdots \times_n (\mathbf{\Phi}_n\mathbf{Q}_n)^\dag \mathbf{\Phi}_n.
\end{equation}
For $0\le n<N$, 
\begin{equation}
\label{eq:recursion}
\mathbb{E} [\|\mathscr{Y}_{n+1}\|_F^2] = \mathbb{E}\left\{ \mathbb{E} [\|\mathscr{Y}_{n+1}\|_F^2 \mid \mathcal{F}_n] \right\} = \mathbb{E} \left\{\mathbb{E}_{\mathbf{Q}_{n+1}, \mathbf{\Phi}_{n+1}}  \|\mathscr{Y}_n\times_{(n+1)}\right (\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}\|_F^2\}. 
\end{equation}
Here, we use the fact that $\mathbf{Q}_{n+1}$ only depends on $\mathbf{\Omega}_{n+1}$. Noticing $\mathscr{Y}_n$, $\mathbf{\Phi}_{n+1}$ and $\mathbf{Q}_{n+1}$ are independent with each other, 
\begin{equation}
\label{eq:conditional_recursion}
\begin{aligned}
&\mathbb{E}_{\mathbf{Q}_{n+1}, \mathbf{\Phi}_{n+1}}  \|\mathscr{Y}_n\times_{(n+1)} (\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}\|_F^2\\
& = \mathbb{E}_{\mathbf{Q}_{n+1}, \mathbf{\Phi}_{n+1}} \|(\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1} \mathbf{Y}_n^{(n+1)}\|_F^2\\
& = \mathbb{E}_{\mathbf{Q}_{n+1}, \mathbf{\Phi}_{n+1}} \|(\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}(\mathbf{Q}_{n+1}\mathbf{Q}_{n+1}^\top +\mathbf{I}-\mathbf{Q}_{n+1}\mathbf{Q}_{n+1}^\top ) \mathbf{Y}_n^{(n+1)}\|_F^2\\
& = \mathbb{E}_{\mathbf{Q}_{n+1}, \mathbf{\Phi}_{n+1}} \|(\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}(\mathbf{Q}_{n+1}\mathbf{Q}_{n+1}^\top)\mathbf{Y}_n^{(n+1)}\|_F^2\\
&+\mathbb{E}_{\mathbf{Q}_{n+1}, \mathbf{\Phi}_{n+1}} \|(\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}(\mathbf{I}-\mathbf{Q}_{n+1}\mathbf{Q}_{n+1}^\top)\mathbf{Y}_n^{(n+1)}\|_F^2\\
&+2\mathbb{E}_{\mathbf{Q}_{n+1}, \mathbf{\Phi}_{n+1}}  \langle(\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}(\mathbf{Q}_{n+1}\mathbf{Q}_{n+1}^\top)\mathbf{Y}_n^{(n+1)},  (\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}(\mathbf{I}-\mathbf{Q}_{n+1}\mathbf{Q}_{n+1}^\top)\mathbf{Y}_n^{(n+1)}\rangle. 
\end{aligned}
\end{equation}
Since $\mathbf{Q}_{n+1}$ is independent of $\mathbf{\Omega}_{n+1}$, the last line in \eqref{eq:conditional_recursion} becomes 
\begin{equation}
\begin{aligned}
&\mathbb{E}_{\mathbf{Q}_{n+1}, \mathbf{\Phi}_{n+1}}  \langle\mathbf{Q}_{n+1}^\top\mathbf{Y}_n^{(n+1)},  (\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}(\mathbf{I}-\mathbf{Q}_{n+1}\mathbf{Q}_{n+1}^\top)\mathbf{Y}_n^{(n+1)}\rangle\\
&= \mathbb{E}_{\mathbf{Q}_{n+1}} \langle\mathbf{Q}_{n+1}^\top\mathbf{Y}_n^{(n+1)}, \mathbb{E}_{\mathbf{\Omega}_{n+1}}  (\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}(\mathbf{I}-\mathbf{Q}_{n+1}\mathbf{Q}_{n+1}^\top)\mathbf{Y}_n^{(n+1)} \mid \mathbf{Q}_{n+1}\rangle. 
\end{aligned}
\end{equation}



Now lemma \ref{lemma:indepence_for_pseduo_inverse} claims that $\mathbf{\Phi}_n\mathbf{Q}_n$ and $\mathbf{\Phi}_n(\mathbf{I} - \mathbf{Q}_n\mathbf{Q}_n^\top)$ are independent given $\mathbf{Q}_n$. Besides, 
$\mathbb{E} \mathbf{\Phi}_{n+1}(\mathbf{I}-\mathbf{Q}_{n+1}\mathbf{Q}_{n+1}^\top )\mid \mathbf{Q}_{n+1} = \mathbf{0}$, then above equation becomes zero. \par 
The fact that the expectation of inner product is zero leads to another Pythagorean Theorem type decomposition of \eqref{eq:conditional_recursion}:
\begin{equation}
\begin{aligned}
&\mathbb{E}_{\mathbf{Q}_{n+1}, \mathbf{\Phi}_{n+1}}  \|\mathscr{Y}_n\times_{(n+1)} (\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}\|_F^2\\
& = \mathbb{E}_{\mathbf{Q}_{n+1}, \mathbf{\Phi}_{n+1}} \|(\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}(\mathbf{Q}_{n+1}\mathbf{Q}_{n+1}^\top)\mathbf{Y}_n^{(n+1)}\|_F^2\\
&+\mathbb{E}_{\mathbf{Q}_{n+1}, \mathbf{\Phi}_{n+1}} \|(\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}(\mathbf{I}-\mathbf{Q}_{n+1}\mathbf{Q}_{n+1}^\top)\mathbf{Y}_n^{(n+1)}\|_F^2.\\
\end{aligned}
\end{equation}
Since $s>k$, $(\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1} = \mathbf{I}_k$, 
\begin{equation}
\|(\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}(\mathbf{Q}_{n+1}\mathbf{Q}_{n+1}^\top)\mathbf{Y}_n^{(n+1)}\|_F^2 = \|\mathbf{Q}_{n+1}^\top\mathbf{Y}_n^{(n+1)}\|_F^2 \le \|\mathbf{Y}_n^{(n+1)}\|_F^2.
\end{equation}
 For the second part, let $\mathbf{B}_{1,n+1} = (\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag$, $\mathbf{B}_{2,n+1} = \mathbf{\Phi}_{n+1}(\mathbf{I}-\mathbf{Q}_{n+1}\mathbf{Q}_{n+1}^\top)$. As we showed above, each element in $\mathbf{B}_{1,n+1}$ is independent from each element in $\mathbf{B}_{2,n+1}$. Then 
\begin{equation}
\begin{aligned}
&\mathbb{E}_{\mathbf{Q}_{n+1}, \mathbf{\Phi}_{n+1}} \|(\mathbf{\Phi}_{n+1}\mathbf{Q}_{n+1})^\dag \mathbf{\Phi}_{n+1}(\mathbf{I}-\mathbf{Q}_{n+1}\mathbf{Q}_{n+1}^\top)\mathbf{Y}_n^{(n+1)}\|_F^2\\
& =\mathbb{E}_{\mathbf{Q}_{n+1}} \mathbb{E}_{\mathbf{B}_{1,n+1}, \mathbf{B}_{2, n+1}} \|\mathbf{B}_{1,n+1}^\dag
\mathbf{B}_{2,n+1}\mathbf{Y}_n^{(n+1)}\|_F^2\\
& = \frac{k}{s-k}\|\mathbf{Y}_n^{(n+1)}\|_F^2.
\end{aligned}
\end{equation}
Then we get a recursive relation as 
\begin{equation}
\mathbb{E}\|\mathscr{Y}_{n}\|_F^2 \le  \left(1+\frac{k}{s-k}\right) \mathbb{E}\|\mathscr{Y}_{n-1}\|_F^2, n\ge 1.
\end{equation}
which leads to 
\begin{equation}
\mathbb{E}\|\mathscr{Y}_{N}\|_F^2 \le  \left(1+\frac{k}{s-k}\right)^N  \mathbb{E}\|\mathscr{Y}_0\|_F^2 \le  \left(1+\frac{k}{s-k}\right)^N  \sum_{n=1}^N \left(1+\frac{k}{k-\rho-1}\right)(\tau^{(n)}_\rho)^2. 
\end{equation}
where the last inequality comes from lemma \ref{lemma: compression_error}. 
\end{proof}
\end{lem}
\fi