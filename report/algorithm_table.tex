\section{More Algorithms}

Higher-order singular value decomposition (HOSVD) is a classical method in finding low-rank structure in tensors \cite{de2000multilinear}. Usually it serves as the initial step for the alternating least square (ALS) in computing the tucker decomposition.  If we assume the tensor to have rank $\mathbf{r} = (r_1, \dots, r_N)$, then the corresponding HOSVD algorithm is given by Algorithm \ref{alg:hosvd}. The HOOI Tucker decomposition (HOSVD and ALS) is given by Algorithm \ref{alg:tucker}. 
\begin{algorithm}[!ht]
\caption{Higher Order SVD}\label{alg:hosvd}
  \begin{algorithmic}[2]
  \Function{HOSVD}{$\mathscr{X}, \mathbf{r}$}
  \For{$n = 1, \dots N$} 
  \State $(\mathbf{U}_n, \cdot, \cdot) \leftarrow \rm{SVD}_{r_n}(\mathbf{X}^{(n)})$
  \EndFor
  \State $\mathscr{S} \leftarrow \mathscr{X}\times_1 \mathbf{U}_1^\top \times \dots \times_N \mathbf{U}_N^\top$
  \State $\hat{\mathscr{X}} \leftarrow \mathscr{S} \times_1 \mathbf{U}_1 \times \cdots \times \mathbf{U}_N $
  \State \Return $(\mathscr{S},\mathbf{U}_1, \dots, \mathbf{U}_N, \hat{\mathscr{X}})$
  \EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!ht]
\caption{HOOI (Higher Order SVD + ALS)} \label{alg:tucker}
\begin{algorithmic}[2]  
\Function{Tucker}{$\mathscr{X},\mathbf{r}$} 
\State $(\mathscr{S},\mathbf{U}_1, \dots, \mathbf{U}_N, \_) = $HOSVD$(\mathscr{X},\mathbf{r})$ 
\Repeat
\For{$n = 1, \dots, N$} 
\State $\mathscr{A} \leftarrow \mathscr{X} \times \mathbf{U}_1^T \times \cdots \times \mathbf{U}_N^T$
\State $(\mathbf{U}_n, \cdot, \cdot)\leftarrow$ $\text{SVD}_{r_n}(\mathbf{A}^{(n)})$
\EndFor
\Until Maximum iteration is reached or convergence
\State $\mathscr{S} \leftarrow \mathscr{X}\times_1 \mathbf{U}_1^\top \times \dots \times_N \mathbf{U}_N^\top$
  \State $\hat{\mathscr{X}} \leftarrow \mathscr{S} \times_1 \mathbf{U}_1 \times \cdots \times \mathbf{U}_N $
  \State \Return $(\mathscr{S},\mathbf{U}_1, \dots, \mathbf{U}_N, \hat{\mathscr{X}})$
\EndFunction 
\end{algorithmic}
\end{algorithm}
As discussed in the previous sections, our one pass algorithms (Algorithm \ref{alg:one_pass_low_rank_appro} and \ref{alg:one_pass_fix_rank_appro}) build on the two-pass low-rank and fixed-rank approximation algorithms, which are given by Algorithm \ref{alg:two_pass_low_rank_appro} and \ref{alg:two_pass_fix_rank_appro}. 
\begin{algorithm}[ht!]
\caption{Two Pass Low-Rank Approximation}\label{alg:two_pass_low_rank_appro}
  \begin{algorithmic}[1]
  \Function{TwoPassLowRankRecovery}{$\mathbf{G}_1, \dots \mathbf{G}_N, \mathscr{X}$}
   \For{$n = 1 \dots N$}
    \State $(\mathbf{Q}_n, \sim) \leftarrow \rm{QR}(\mathbf{G}_n)$
   \EndFor
   \State $\hat{\mathscr{X}} \leftarrow \mathscr{X} \times_1 \mathbf{Q}_1\mathbf{Q}_1^\top \dots \times_N \mathbf{Q}_N\mathbf{Q}_N^\top$
   \State \Return $\hat{\mathscr{X}}$
   \EndFunction
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[ht!] 
\begin{algorithmic}[1]
\caption{Two Pass Fixed-Rank Approximation}\label{alg:two_pass_fix_rank_appro}
\Function{TwoPassFixRankRecovery}{$\mathbf{G}_1, \dots \mathbf{G}_N, \mathscr{X}, \mathbf{r}$}
\For{$n = 1 \dots N$}
\State $(\mathbf{Q}_n, \sim) \leftarrow \rm{QR}(\mathbf{G}_n)$
\EndFor
\State $\mathscr{W} \leftarrow \mathscr{X} \times_1 \mathbf{Q^\top}_1 \times \cdots \times_N \mathbf{Q}_N^\top$ 
\State $\hat{\mathscr{X}} \leftarrow \llbracket \mathscr{W} \rrbracket _{\mathbf{r}} \times_1 \mathbf{Q}_1 \dots \times_N \mathbf{Q}_N$
\State \Return $\hat{\mathscr{X}}$
\EndFunction
\end{algorithmic}
\end{algorithm}

