\section{Computational Complexity} \label{appendix: time-complexity}

\paragraph{Flops for Batch}
In the sketching stage of the two-pass algorithms, we need to first compute the arm sketches, $\mathbf{G}_n = \mathbf{X}\mathbf{\Omega}_n, n \in [N]$ with $kN\hat{I}$ flops in total. Then, in the recovery stage, we first perform "economy size" QR factorizations on $\mathbf{G}_1, \dots, \mathbf{G}_N$ with $\mathscr{O}(k^2(\sum_{n =1}^N I_n))$ to find the orthonormal bases $\mathbf{Q}_1, \dots, \mathbf{Q}_N$. Then, we compute the linkage tensor $\mathscr{W}$ by recursively multiplying $\mathscr{X}$ by $\mathbf{Q}_n^\top, n \in [N]$, with $(k\cdot I_1 \cdot I_{(-1)} + k \cdot I_2 \cdot \frac{I_{(-2)}\cdot k}{I_1} + \cdots + k^{N}\cdot I_N)$ flops, which is bounded by $\mathcal{O}(\frac{k(1-\delta_1^N)\bar{I}}{1-\delta_1})$. The multiplication step together with the arm sketch computation dominates the total cost as in Table \ref{tbl: time-complexity}. 

In the sketching case, the one-pass algorithm needs to additionally compute the core tensor sketch $\mathscr{Z}$ by recursively multiplying $\mathscr{X}$ by $\mathbf{\Phi}_n, n \in [N]$. We can find the upper bound for the number of flops to be $\frac{s(1-\delta_1^N)}{1-\delta_1}\bar{I}$. Finding the orthonormal basis of the arm sketch takes $\mathscr{O}(k^2(\sum_{n =1}^N I_n))$ similar to the two-pass algorithm. To find the linkage tensor $\mathscr{W}$, we need to recursively solve linear square problems, with $\frac{k^2s^N(1-(k/s)^N)}{1-k/s}$ flops. The sketch computation dominates the total time complexity, which is higher than the total time cost of the two-pass algorithm with a different compression factor.

The higher order SVD directly acts on $\mathscr{X}$ by first computing the SVD for each unfolding in $\mathscr{O}(kN\bar{I})$, and then multiplying $\mathscr{X}$ by $\mathbf{U}_1^\top, \dots, \mathbf{U}_N^\top$ in $\mathcal{O}(\frac{k(1-\delta_1^N)\bar{I}}{1-\delta_1})$. The total time cost is approximately the same as the two-pass algorithm. Note: we can use the randomized SVD in the first step to improve the computational cost to $\bar{I}N\log k + \sum_{n = 1}^N(I_{n}+I_{(-n)})k^2$ \cite{halko2011finding}. 

\paragraph{Flops for Streaming} Using the elementwise representation of the mode product, we can significantly simplify the computation for sparse input tensor in the streaming model. In computing the arm sketches for both one-pass and two-pass algorithm, the complexity goes from $kN\bar{I}$ to $\mu kN\bar{I}$. Furthermore, we can reduce the computational complexity of multiplying $\mathscr{X}$ by $\mathbf{Q}_1^T, \dots, \mathbf{Q}_N^T$ from $\mathscr{O}(\frac{k(1-\delta_1^N)\bar{I}}{1-\delta_1})$ to $\mathscr{O}(\mu k^N N(\sum_{n =1}^N I_n ))$ in the recovery stage of the two-pass algorithm. Similarly, we can reduce the complexity of multiplying $\mathscr{X}$ by $\mathbf{\Phi}_1, \dots, \mathbf{\Phi}_N$ from $\frac{s(1-\delta_2^N)\bar{I}}{1-\delta_2}$ to $\mu s^NN(\sum_{n=1}^NI_n)$ in the sketching phase of one-pass algorithm. The higher order SVD, in comparison, does not support the streaming model. 
