\section{Probabilistic Analysis of Core Sketch Error}
Let us introduce the orthonormal matrix $\mathbf{Q}_n^\bot$ whose range is complementary to that of $\mathbf{Q}_n$ for each $n$, i.e., $\mathbf{Q}_n^\bot (\mathbf{Q}_n^\bot)^\top = \mathbf{I} - \mathbf{Q}_n\mathbf{Q}_n^\top$. Next, we denote 
\begin{equation}
\begin{aligned}
\mathbf{\Phi}_n^Q = \mathbf{\Phi}_n \mathbf{Q}_n  ~~~~\mathbf{\Phi}_n^{Q^\bot} = \mathbf{\Phi}_n \mathbf{Q}_n^\bot.  
\end{aligned}
\end{equation}
Apparently, conditional on $\mathbf{Q}_n$, $\mathbf{\Phi}_n^Q$ and $\mathbf{\Phi}_n^{Q^\bot}$ are independent of each other. 

\subsection{Decomposition of Core Sketch Error}
This session, we show a decomposition formula for core sketch error.  
\begin{lem}
\label{lemma:core_error_decomposition}
Assume $\mathbf{\Phi}_n$ has full column rank (this holds with probability 1 in fact). Then
\begin{equation}
\mathscr{W} - \mathscr{X}\times_1 \mathbf{Q}_1^\top \dots \times_N \mathbf{Q}_N^\top = 
\sum_{(i_1,\dots, i_N) \in \{0,1\}^N, \prod_{j=1}^N i_j \neq 0} \mathscr{Y}_{i_1\dots i_N}, 
\end{equation}
where 
\begin{equation}
\label{eq:def_each_part}
\begin{aligned}
\mathscr{Y}_{i_1\dots i_N} &= \mathscr{X}\times_1 \left(\mathbbm{1}_{i_1=0}\mathbf{Q}_1^\top + \mathbbm{1}_{i_1=1}(\mathbf{\Phi}_1^Q)^\dag  \mathbf{\Phi}_1^{Q^\bot}(\mathbf{Q}_1^\bot)^\top) \right)\\
&\times_2 \dots \times_N \left(\mathbbm{1}_{i_N=0}\mathbf{Q}_N^\top + \mathbbm{1}_{i_1=1}(\mathbf{\Phi}_N^Q)^\dag  \mathbf{\Phi}_N^{Q^\bot}(\mathbf{Q}_N^\bot)^\top) \right).
\end{aligned}
\end{equation}
\end{lem}


\begin{proof}
We could write $\mathscr{W}$ as 
\begin{equation}
\begin{aligned}
&\mathscr{W} = \mathscr{Z}\times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times_2 \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag \\
& = (\mathscr{X} -  \tilde{\mathscr{X}})\times_1 \mathbf{\Phi}_1 \times_2 \cdots \times_N \mathbf{\Phi}_N  \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times_2 \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag 
\\
&+ \tilde{\mathscr{X}}\times_1 \mathbf{\Phi}_1 \times_2 \cdots \times_N \mathbf{\Phi}_N \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times_2 \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag.
\end{aligned}
\end{equation}
We could simplify the second term as 
\begin{equation}
\begin{aligned}
&\tilde{\mathscr{X}}\times_1 \mathbf{\Phi}_1 \times_2 \cdots \times_N \mathbf{\Phi}_N \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times_2 \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag   \\
& = \mathscr{X}\times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \mathbf{\Phi}_1\mathbf{Q}_1\mathbf{Q}_1^\top \times_2 \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag \mathbf{\Phi}_N\mathbf{Q}_N\mathbf{Q}_N^\top\\
& = \mathscr{X}\times_1 \mathbf{Q}_1^\top \times_2 \cdots \times_N \mathbf{Q}_N^\top.
\end{aligned}
\end{equation}
The last equation comes from the fact that for a matrix $\mathbf{A}$ with shape $s\times k$, if $s>k$, $A^\dag A = I_k$. Therefore
\begin{equation}
\begin{aligned}
&\mathscr{W} = (\mathscr{X} -  \tilde{\mathscr{X}})\times_1 \mathbf{\Phi}_1 \times_2 \cdots \times_N \mathbf{\Phi}_N  \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times_2 \cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag \\
&+ \mathscr{X}\times_1 \mathbf{Q}_1^\top \times_2 \cdots \times_N \mathbf{Q}_N^\top.
\end{aligned}
\end{equation}
Then the error could be decomposed as 
\begin{equation}
\label{eq:core_err_decom}
\begin{aligned}
&(\mathscr{X} -  \tilde{\mathscr{X}})\times_1 \mathbf{\Phi}_1 \times_2 \cdots \times_N \mathbf{\Phi}_N  \times_1 (\mathbf{\Phi}_1 \mathbf{Q}_1)^\dag \times_2\cdots \times_N (\mathbf{\Phi}_N \mathbf{Q}_N)^\dag \\
& =(\mathscr{X} -  \tilde{\mathscr{X}})\times_1 (\mathbf{\Phi}_1\mathbf{Q}_1)^\dag \mathbf{\Phi}_1 \times_2\cdots \times_N (\mathbf{\Phi}_N\mathbf{Q}_N)^\dag \mathbf{\Phi}_N \\
& =  (\mathscr{X} -  \tilde{\mathscr{X}}) \times_1 (\mathbf{\Phi}_1\mathbf{Q}_1)^\dag \mathbf{\Phi}_1(\mathbf{Q}_1\mathbf{Q}_1^\top + \mathbf{Q}_1^\bot (\mathbf{Q}_1^\bot)^\top)\dots \times_N (\mathbf{\Phi}_N\mathbf{Q}_N)^\dag \mathbf{\Phi}_N(\mathbf{Q}_N\mathbf{Q}_N^\top + \mathbf{Q}_N^\bot (\mathbf{Q}_N^\bot)^\top)\\
& = (\mathscr{X} -  \tilde{\mathscr{X}}) \times_1 (\mathbf{Q}_1^\top + (\mathbf{\Phi}_1^Q)^\dag  \mathbf{\Phi}_1^{Q^\bot}(\mathbf{Q}_1^\bot)^\top)\times_2\dots \times_N (\mathbf{Q}_N^\top + (\mathbf{\Phi}_N^Q)^\dag  \mathbf{\Phi}_N^{Q^\bot}(\mathbf{Q}_N^\bot)^\top).
\end{aligned}
\end{equation}
We need to sum up these $2^N-1$ terms. To simply the summation, we claim followings:
\begin{enumerate}
\item $(\mathscr{X} - \tilde{\mathscr{X}})\times_1 \mathbf{Q}_1^\top\dots \times_N \mathbf{Q}_N^\top = 0$.
\item for any $1\le n\le N$, $\tilde{\mathscr{X}}\times_n (\mathbf{\Phi}_n^Q)^\dag  \mathbf{\Phi}_n^{Q^\bot}(\mathbf{Q}_n^\bot)^\top =  0$.
\end{enumerate}
Here $0$ means a tensor with all zero elements. There two claims could be got straightly from exchange rule of mode product given by \ref{eq: tensor_product_mul_exchangable}. Then follow the definition of $\mathscr{Y}_{i_1\dots i_N}$ in \eqref{eq:def_each_part},
we could write \eqref{eq:core_err_decom} as 
\begin{equation}
\sum_{(i_1,\dots, i_N) \in \{0,1\}^N, \prod_{j=1}^N i_j \neq 0} \mathscr{Y}_{i_1\dots i_N}
\end{equation}
to complete the proof.
\end{proof}
\subsection{Probabilistic Core Error Bound}
In this section, we derive the probabilistic error bound based on the core error decomposition shown in lemma \ref{lemma:core_error_decomposition}. 
\begin{lem}
\label{lemma:err_core_sketch}
If $s>2k$, each element of $\mathbf{\Phi}_n, \mathbf{\Omega}_n$ is from standard Gaussian distribution for all $n, n\in [N]$, then for any natural number $1\le \rho<k$, 
\begin{equation}
\mathbb{E} \|\mathscr{W} - \mathscr{X}\times_1 \mathbf{Q}_1^\top \dots \times_N \mathbf{Q}_N^\top\|_F^2 \le \frac{k}{s-k-1} \left[ \sum_{n=1}^N \left(1+\frac{\rho}{k-\rho-1}\right)(\tau^{(n)}_\rho)^2\right].
\end{equation}
\end{lem}
\begin{proof}
It suffices to show that 
\begin{equation}
\mathbb{E}\left[ \|\mathscr{W} - \mathscr{X}\times_1 \mathbf{Q}_1^\top \dots \times_N \mathbf{Q}_N^\top\|_F^2 \mid \mathbf{\Omega}_1, \cdots, \mathbf{\Omega}_N \right] \le \frac{k}{s-k-1}  \|\mathscr{X} - \tilde{\mathscr{X}}\|_F^2, 
\end{equation}
then we could take an expectation with respect to $\mathbf{\Omega}_1, \cdots,  \mathbf{\Omega}_N$ and apply lemma \ref{lemma: compression_error} to finish the proof. Here we use the fact that $\{\mathbf{\Omega_n}, 1\le n\le N\}$ are independent with $\{\mathbf{\Phi_n}, 1\le n\le N\}$ and randomness of $\mathbf{Q}_n$ solely comes from $\mathbf{\Omega}_n$. \par 
Lemma \ref{lemma:core_error_decomposition} decompose the core error as summantion of $\mathscr{Y}_{i_1\cdots i_n}$ on the set $\{i_j\in \{0,1\}, \prod_{j=1}^N i_j \neq 0\}$. We will first show the upper bound for each $\mathscr{Y}_{i_1\cdots i_n}$. Applying lemma \ref{lemma:sketchy_column_space_err}, and noticing on the index set, $\sum_{j=1}^N i_j \ge 1$,  
\begin{equation}
\mathbb{E} \left[ \|\mathscr{Y}_{i_1\dots i_N}\|_F^2 \mid \mathbf{\Omega}_1 \cdots \mathbf{\Omega}_N \right] = \left(\frac{k}{s-k-1}\right)^{\sum_{j=1}^N i_j} \|\mathscr{B}_{i_1\dots i_N}\|_F^2 \le \left(\frac{k}{s-k-1}\right) \|\mathscr{B}_{i_1\dots i_N}\|_F^2,
\end{equation}
where we define $\mathscr{B}_{i_1\dots i_N}$ as
\begin{equation}
\begin{aligned}
&\mathscr{B}_{i_1\dots i_N} = \\
&\mathscr{X}\times_1 (\mathbbm{1}_{i_1=0}\mathbf{Q}_1\mathbf{Q}_1^\top + \mathbbm{1}_{i_1=1}\mathbf{Q}_1^\bot(\mathbf{Q}_1^\bot)^\top)\cdots\times_N(\mathbbm{1}_{i_N=0}\mathbf{Q}_N\mathbf{Q}_N^\top + \mathbbm{1}_{i_N=1}\mathbf{Q}_N^\bot(\mathbf{Q}_N^\bot)^\top).
\end{aligned}
\end{equation}
Here we use the fact that mode product with orthogonal matrix does not change the F norm:
\begin{equation}
\begin{aligned}
&\left\|\mathscr{X}\times_1 (\mathbbm{1}_{i_1=0}\mathbf{Q}_1\mathbf{Q}_1^\top + \mathbbm{1}_{i_1=1}\mathbf{Q}_1^\bot(\mathbf{Q}_1^\bot)^\top)\cdots\times_N(\mathbbm{1}_{i_N=0}\mathbf{Q}_N\mathbf{Q}_N^\top + \mathbbm{1}_{i_N=1}\mathbf{Q}_N^\bot(\mathbf{Q}_N^\bot)^\top)\right\|_F \\
&= \left\|\mathscr{X}\times_1 (\mathbbm{1}_{i_1=0}\mathbf{Q}_1^\top + \mathbbm{1}_{i_1=1}(\mathbf{Q}_1^\bot)^\top)\cdots\times_N(\mathbbm{1}_{i_N=0}\mathbf{Q}_N^\top + \mathbbm{1}_{i_N=1}(\mathbf{Q}_N^\bot)^\top)\right\|_F.
\end{aligned}
\end{equation}

Now we show that inner product of two different $\mathscr{B}_{i_1\cdots i_N}$ is zero. Consider $q_1, q_2 \in \{0,1\}^N$ be the index(binary) vector of length $N$. For different index $q_1, q_2$, at least there exists $1\le r\le N$, such that their r-th element is different. Without loss of generality, $q_1(r) = 0$ and  $q_2(r)=1$, \begin{equation}\label{eq:inner_prod2}
\langle \mathscr{B}_{q_1}, \mathscr{B}_{q_2}\rangle = \langle  \mathbf{B}^{(r)}_{q_1}, \mathbf{B}^{(r)}_{q_2} \rangle = \langle \dots \mathbf{Q}_r^\top \mathbf{Q}_r^\bot \dots\rangle  = 0. 
\end{equation}
Noticing 
\begin{equation}
\begin{aligned}
&\sum_{(i_1,\dots, i_N) \in \{0,1\}^N} \mathscr{B}_{i_1\dots i_N} = \mathscr{X},  
\end{aligned}
\end{equation}
and $\mathscr{B}_{1,\cdots, 1} = \tilde{\mathscr{X}}$ (with all $i_n=1$ ), then by Pythagorean theorem, we have
\begin{equation}
\sum_{(i_1,\dots, i_N) \in \{0,1\}^N, \prod_{j=1}^N i_j \neq 0} \|\mathscr{B}_{i_1\dots i_N}\|_F^2 = \|\mathscr{X} - \tilde{\mathscr{X}}\|_F^2. 
\end{equation}
Putting all these together, 
\begin{equation}
\begin{aligned}
&\mathbb{E}\left[ \|\mathscr{W} - \mathscr{X}\times_1 \mathbf{Q}_1^\top \dots \times_N \mathbf{Q}_N^\top\|_F^2 \mid \mathbf{\Omega}_1, \cdots, \mathbf{\Omega}_N \right] \\
& = \sum_{(i_1,\dots, i_N) \in \{0,1\}^N, \prod_{j=1}^N i_j \neq 0} \mathbb{E} \left[\|\mathscr{Y}_{i_1\dots i_N}\|_F^2 \mid \mathbf{\Omega_1}, \dots, \mathbf{\Omega}_N\right]\\
&\le \frac{k}{s-k-1} \left(\sum_{(i_1,\dots, i_N) \in \{0,1\}^N, \prod_{j=1}^N i_j \neq 0} \|\mathscr{B}_{i_1\dots i_N}\|_F^2 \right)\\
&= \frac{k}{s-k-1} \|\mathscr{X} - \tilde{\mathscr{X}}\|_F^2. 
\end{aligned}
\end{equation}
Noticing no $\mathbf{\Omega}_n$ involved in above bound, taking expectation on $\mathbf{\Omega}_n$ completes the proof. 
\end{proof}


